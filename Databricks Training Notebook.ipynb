{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52c19e40-51fe-4534-902b-4d9f4af513e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Training: Oil & Gas Data Analysis with WellView Data\n",
    "\n",
    "**Welcome to Databricks!** This 1-hour beginner training will teach you Databricks notebook fundamentals, data engineering basics, and visualization capabilities using realistic oil & gas well data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a24d4db-9d38-426f-a1f0-80afa71f7784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 1: Databricks Notebook Basics (10 minutes)\n",
    "\n",
    "### What is Databricks?\n",
    "\n",
    "Databricks is a unified analytics platform built on Apache Spark that allows you to process large-scale data, build data pipelines, and create machine learning models. At its core, Databricks notebooks provide an interactive environment for data analysis combining code, visualizations, and documentation.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Notebooks** contain cells that can execute code, SQL, or display markdown\n",
    "- **Clusters** provide the compute resources to execute your code\n",
    "- **Unity Catalog** provides centralized data governance with a three-level namespace: `catalog.schema.table`\n",
    "- **Delta Lake** is the default storage format, providing ACID transactions and time travel\n",
    "\n",
    "### Magic Commands\n",
    "\n",
    "Magic commands are special instructions that change how Databricks interprets a cell. They always start with `%` and must be the first thing in a cell.\n",
    "\n",
    "**Essential Magic Commands:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52e39acd-36c5-40e0-809c-88fefb443975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "print(\"Python is the default language in Python notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75f6fc2a-2226-4984-bef3-182886298a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Available Magic Commands:**\n",
    "- `%python` - Execute Python code (default in Python notebooks)\n",
    "- `%sql` - Execute SQL queries directly\n",
    "- `%scala` - Execute Scala code\n",
    "- `%r` - Execute R code\n",
    "- `%sh` - Run shell commands on the driver node\n",
    "- `%fs` - Use filesystem commands (e.g., `%fs ls /path/`)\n",
    "- `%md` - Render Markdown for documentation\n",
    "- `%run` - Execute another notebook\n",
    "\n",
    "**Try It Yourself:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cccba832-5464-45b4-a305-fc4a268931f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- #TODO: Change this query to show all schemas in the catalog\n",
    "SHOW CATALOGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e2bd1d9-b06b-4da2-b25f-0b73389c24aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SOLUTION Show Schemas"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW SCHEMAS IN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b073a4-bc31-4824-aa1d-ce5b998cb0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "#TODO: Run a shell command to check Python version\n",
    "echo \"Running on Databricks!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb52652-ffd6-4913-83a7-c139ecc786c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SOLUTION Python Version"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f91d0d0d-bb28-4536-b2ea-14f400da11e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Markdown Best Practices\n",
    "\n",
    "Use markdown cells (`%md`) to document your notebooks with headers, lists, code formatting, and more.\n",
    "\n",
    "**Markdown Syntax Examples:**\n",
    "- `# Header 1`, `## Header 2`, `### Header 3`\n",
    "- `**bold text**` and `*italic text*`\n",
    "- `` `code formatting` `` for inline code\n",
    "- Bullet lists with `-` or `*`\n",
    "- Numbered lists with `1.`, `2.`, etc.\n",
    "- Tables using `|` separators\n",
    "- Links: `[Link Text](URL)`\n",
    "\n",
    "**Best Practices for Comments:**\n",
    "1. Use markdown cells for section descriptions and context\n",
    "2. Use `#` comments in code cells for line-by-line explanation\n",
    "3. Use `#TODO` to mark exercises for trainees\n",
    "4. Use `#NOTE` for important callouts\n",
    "5. Use docstrings for function definitions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7848cdd3-542e-4e9e-b5cb-c7909d284da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Markdown TODO\n",
    "Create a markdown cell below with header 4, a code snippet and a bulleted list of details explaining the code snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7c6de03-182f-49cc-bfcb-ecbe3c50fdba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Section 2: Unity Catalog Setup (10 minutes)\n",
    "\n",
    "Unity Catalog provides centralized governance and a three-level namespace for organizing data: **catalog.schema.table**\n",
    "\n",
    "### Understanding the Three-Level Namespace\n",
    "\n",
    "**Catalogs** are the top-level containers (often organized by environment or business unit)\n",
    "↓\n",
    "**Schemas** provide additional organization within catalogs (often by team or project)\n",
    "↓\n",
    "**Tables** store your actual data\n",
    "\n",
    "**Example:** `training_catalog.wellview_data.well_headers`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fba523c-fab0-4fe1-8084-e70f296c2079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Setting Up Your Training Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7171722-62f9-436f-ab42-f2926c6f932a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import DeltaTable\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Get current user for personalized schema naming\n",
    "current_user = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "# Clean username for use in schema name (remove special characters)\n",
    "clean_username = current_user.split('@')[0].replace('.', '_').replace('-', '_')\n",
    "\n",
    "print(f\"Welcome, {current_user}!\")\n",
    "print(f\"Your schema will be: {clean_username}_wellview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfae5386-199c-4d5b-8bf4-9082bfcc4dd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- View available catalogs\n",
    "SHOW CATALOGS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ddee1f-9f2e-4495-844c-ea53d2d8734c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- #TODO: Replace 'training' with your assigned catalog name\n",
    "USE CATALOG training;\n",
    "\n",
    "-- View schemas in this catalog\n",
    "SHOW SCHEMAS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19d8a32b-9e81-4b64-b4db-0d4c34dbe450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create your personal schema for this training\n",
    "# Each trainee gets their own schema to work in\n",
    "\n",
    "# #TODO: Update the catalog name if instructed by your trainer\n",
    "training_catalog = \"training\"\n",
    "your_schema = f\"{clean_username}_wellview\"\n",
    "\n",
    "# Create schema\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {training_catalog}.{your_schema}\")\n",
    "spark.sql(f\"USE SCHEMA {your_schema}\")\n",
    "\n",
    "print(f\"✓ Created and using schema: {training_catalog}.{your_schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d2045b-a220-4a4f-a84c-0f25849d8371",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verify your current context\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Current Catalog: {current_catalog}\")\n",
    "print(f\"Current Schema: {current_schema}\")\n",
    "print(f\"Full namespace: {current_catalog}.{current_schema}.table_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1383e43f-a6c4-4b73-8230-6c5061eabf59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 3: Generate WellView Training Data (10 minutes)\n",
    "\n",
    "WellView is commercial well information management software used by over 70% of the oil & gas industry. We'll create realistic fake data mimicking a WellView system with three key tables: well headers, production data, and completion data.\n",
    "\n",
    "### Understanding Oil & Gas Data\n",
    "\n",
    "**API Well Number:** 14-digit unique identifier (XX-XXX-XXXXX-XX-XX)\n",
    "- State code, county code, well number, sidetrack code, event sequence\n",
    "\n",
    "**Production Volumes:**\n",
    "- Oil: Measured in barrels (BBL), typically 10-2,000+ bbl/day\n",
    "- Gas: Measured in thousand cubic feet (MCF), typically 100-10,000+ MCF/day\n",
    "- Water: Measured in barrels, increases over well lifetime\n",
    "\n",
    "**Common Formations:** Bakken, Eagle Ford, Marcellus, Permian Basin, Haynesville\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "768d3bbd-33ce-4033-b7cb-d4dbdbf591c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Generate Well Headers Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9c8d32-f313-492e-a7b9-e3b2bac915a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate realistic well header data\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Define state/county codes for Texas\n",
    "state_code = 42  # Texas\n",
    "county_codes = [453, 415, 329, 441, 165]  # Travis, Tom Green, Midland, Taylor, Gaines\n",
    "\n",
    "# Generate formations commonly found in Texas\n",
    "formations = [\"Wolfcamp\", \"Spraberry\", \"Bone Spring\", \"Wolfberry\", \"Delaware\", \n",
    "              \"Eagle Ford\", \"Austin Chalk\", \"Barnett Shale\"]\n",
    "\n",
    "# Generate operators (fake company names)\n",
    "operators = [\"Permian Resources\", \"Eagle Oil Co\", \"Texas Energy Partners\", \n",
    "             \"Lone Star Exploration\", \"Wolfcamp Operating LLC\", \"Shale Development Inc\"]\n",
    "\n",
    "# Generate well data\n",
    "well_data = []\n",
    "num_wells = 100\n",
    "\n",
    "for i in range(num_wells):\n",
    "    county = random.choice(county_codes)\n",
    "    well_num = random.randint(20001, 89999)  # Current well numbers\n",
    "    sidetrack = random.choice([0, 0, 0, 1, 2])  # Most wells are original bore\n",
    "    \n",
    "    api_number = f\"{state_code:02d}-{county:03d}-{well_num:05d}-{sidetrack:02d}-00\"\n",
    "    \n",
    "    # Generate spud date (well start date) between 2015-2024\n",
    "    spud_date = datetime(2015, 1, 1) + timedelta(days=random.randint(0, 3650))\n",
    "    \n",
    "    # Completion date 30-90 days after spud\n",
    "    completion_date = spud_date + timedelta(days=random.randint(30, 90))\n",
    "    \n",
    "    # Generate well name\n",
    "    well_name = f\"Well #{random.randint(1, 999):03d}\"\n",
    "    lease_name = f\"{random.choice(['Smith', 'Johnson', 'Williams', 'Brown', 'Jones'])} Ranch\"\n",
    "    full_well_name = f\"{random.choice(operators)} {well_name} {lease_name}\"\n",
    "    \n",
    "    # Well type - more horizontal wells in recent years\n",
    "    well_type = random.choice([\"Vertical\", \"Horizontal\"])\n",
    "    \n",
    "    # Total depth\n",
    "    if well_type == \"Horizontal\":\n",
    "        total_depth = random.randint(8000, 15000)\n",
    "        lateral_length = random.randint(4000, 10000)\n",
    "    else:\n",
    "        total_depth = random.randint(5000, 12000)\n",
    "        lateral_length = 0\n",
    "    \n",
    "    # Status\n",
    "    status = random.choice([\"Producing\", \"Producing\", \"Producing\", \"Shut-In\", \"Plugged & Abandoned\"])\n",
    "    \n",
    "    # Location (fake coordinates in Texas)\n",
    "    lat = random.uniform(31.0, 33.0)\n",
    "    long = random.uniform(-103.0, -100.0)\n",
    "    \n",
    "    well_data.append({\n",
    "        \"api_number\": api_number,\n",
    "        \"well_name\": full_well_name,\n",
    "        \"operator\": random.choice(operators),\n",
    "        \"well_type\": well_type,\n",
    "        \"status\": status,\n",
    "        \"spud_date\": spud_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"completion_date\": completion_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"total_depth_ft\": total_depth,\n",
    "        \"lateral_length_ft\": lateral_length,\n",
    "        \"target_formation\": random.choice(formations),\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": long,\n",
    "        \"county_code\": county,\n",
    "        \"state\": \"Texas\"\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "well_headers_df = spark.createDataFrame(well_data)\n",
    "\n",
    "# Display the data\n",
    "print(f\"Generated {num_wells} well records\")\n",
    "display(well_headers_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caec6604-9f4e-453c-b4f2-1df453af471e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write to Delta table\n",
    "table_name = f\"{training_catalog}.{your_schema}.well_headers\"\n",
    "\n",
    "well_headers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"✓ Created table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd3a37d-d636-49eb-bcbd-b621bce27693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Production Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7d3cc1d-17d8-4593-b993-eeb5385f4e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate production data for each well\n",
    "# Create multiple months of production for each well\n",
    "# NOTE __builtins__ allows the use of the python built-in function vs pyspark equivalent\n",
    "\n",
    "production_data = []\n",
    "\n",
    "# Read wells back\n",
    "wells = spark.table(f\"{training_catalog}.{your_schema}.well_headers\").collect()\n",
    "\n",
    "for well in wells:\n",
    "    api_number = well.api_number\n",
    "    completion_date = datetime.strptime(well.completion_date, \"%Y-%m-%d\")\n",
    "    well_status = well.status\n",
    "    well_type = well.well_type\n",
    "    \n",
    "    # Generate 1-3 years of production data\n",
    "    if well_status == \"Plugged & Abandoned\":\n",
    "        months_producing = random.randint(12, 24)\n",
    "    elif well_status == \"Shut-In\":\n",
    "        months_producing = random.randint(6, 18)\n",
    "    else:\n",
    "        months_producing = random.randint(12, 36)\n",
    "    \n",
    "    # Initial production rates (higher for horizontal wells)\n",
    "    if well_type == \"Horizontal\":\n",
    "        initial_oil = random.randint(300, 1500)\n",
    "        initial_gas = random.randint(1000, 8000)\n",
    "        initial_water = random.randint(10, 100)\n",
    "    else:\n",
    "        initial_oil = random.randint(50, 300)\n",
    "        initial_gas = random.randint(200, 2000)\n",
    "        initial_water = random.randint(5, 50)\n",
    "    \n",
    "    # Generate monthly production with decline\n",
    "    for month in range(months_producing):\n",
    "        production_date = completion_date + timedelta(days=30 * month)\n",
    "        \n",
    "        # Decline curve (exponential decline)\n",
    "        decline_factor = 0.85 ** (month / 12)  # 15% annual decline\n",
    "        \n",
    "        # Add some random variation\n",
    "        variation = random.uniform(0.85, 1.15)\n",
    "        \n",
    "        oil_volume = __builtins__.max(5, int(initial_oil * decline_factor * variation))\n",
    "        gas_volume = __builtins__.max(50, int(initial_gas * decline_factor * variation))\n",
    "        \n",
    "        # Water production increases over time (water cut increases)\n",
    "        water_increase = 1 + (month / 12) * 0.5\n",
    "        water_volume = int(initial_water * water_increase * variation)\n",
    "        \n",
    "        # Days on production (assume full month unless shut-in)\n",
    "        days_on = 30 if random.random() > 0.05 else random.randint(15, 29)\n",
    "        \n",
    "        # Calculate GOR (Gas-Oil Ratio)\n",
    "        gor = __builtins__.round(gas_volume / oil_volume * 1000, 1) if oil_volume > 0 else 0\n",
    "        \n",
    "        # Calculate water cut\n",
    "        total_liquid = oil_volume + water_volume\n",
    "        water_cut = __builtins__.round((water_volume / total_liquid * 100), 1) if total_liquid > 0 else 0\n",
    "        \n",
    "        production_data.append({\n",
    "            \"api_number\": api_number,\n",
    "            \"production_date\": production_date.strftime(\"%Y-%m-%d\"),\n",
    "            \"oil_volume_bbl\": oil_volume,\n",
    "            \"gas_volume_mcf\": gas_volume,\n",
    "            \"water_volume_bbl\": water_volume,\n",
    "            \"days_on_production\": days_on,\n",
    "            \"gas_oil_ratio\": gor,\n",
    "            \"water_cut_pct\": water_cut\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "production_df = spark.createDataFrame(production_data)\n",
    "\n",
    "print(f\"Generated {len(production_data):,} production records\")\n",
    "display(production_df.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d07386ac-7d64-4553-b7ed-86c06c788038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write production data to Delta table\n",
    "production_table = f\"{training_catalog}.{your_schema}.production_data\"\n",
    "\n",
    "production_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(production_table)\n",
    "\n",
    "print(f\"✓ Created table: {production_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf975a5d-3e35-4a0f-aaf2-889430e7b289",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Generate Completion Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65ab845a-6813-4584-95ca-4e080233ea14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Generate completion data for each well\n",
    "completion_data = []\n",
    "\n",
    "completion_methods = [\n",
    "    \"Hydraulic Fracture - Multi-Stage\",\n",
    "    \"Hydraulic Fracture - Single Stage\", \n",
    "    \"Acid Stimulation\",\n",
    "    \"Natural Completion\",\n",
    "    \"Gravel Pack\"\n",
    "]\n",
    "\n",
    "proppant_types = [\"White Sand\", \"Brown Sand\", \"Ceramic\", \"Resin Coated Sand\"]\n",
    "\n",
    "for well in wells:\n",
    "    api_number = well.api_number\n",
    "    completion_date = well.completion_date\n",
    "    well_type = well.well_type\n",
    "    formation = well.target_formation\n",
    "    total_depth = well.total_depth_ft\n",
    "    lateral_length = well.lateral_length_ft\n",
    "    \n",
    "    # Horizontal wells typically use multi-stage hydraulic fracturing\n",
    "    if well_type == \"Horizontal\":\n",
    "        method = \"Hydraulic Fracture - Multi-Stage\"\n",
    "        num_stages = random.randint(15, 40)\n",
    "        fluid_volume = random.randint(150000, 500000)  # gallons per stage\n",
    "        proppant_volume = random.randint(200000, 800000)  # lbs total\n",
    "    else:\n",
    "        method = random.choice(completion_methods)\n",
    "        num_stages = 1 if \"Single\" in method else random.randint(1, 5)\n",
    "        fluid_volume = random.randint(10000, 100000)\n",
    "        proppant_volume = random.randint(20000, 200000)\n",
    "    \n",
    "    # Perforated interval\n",
    "    perf_top = total_depth - random.randint(100, 500)\n",
    "    perf_bottom = total_depth\n",
    "    \n",
    "    # Maximum treatment pressure\n",
    "    max_pressure = random.randint(5000, 10000)\n",
    "    \n",
    "    completion_data.append({\n",
    "        \"api_number\": api_number,\n",
    "        \"completion_date\": completion_date,\n",
    "        \"completion_method\": method,\n",
    "        \"formation\": formation,\n",
    "        \"num_stages\": num_stages,\n",
    "        \"total_fluid_volume_gal\": fluid_volume * num_stages,\n",
    "        \"total_proppant_lbs\": proppant_volume,\n",
    "        \"proppant_type\": random.choice(proppant_types),\n",
    "        \"perf_top_depth_ft\": perf_top,\n",
    "        \"perf_bottom_depth_ft\": perf_bottom,\n",
    "        \"max_treatment_pressure_psi\": max_pressure,\n",
    "        \"lateral_length_ft\": lateral_length\n",
    "    })\n",
    "\n",
    "# Create DataFrame  \n",
    "completion_df = spark.createDataFrame(completion_data)\n",
    "\n",
    "print(f\"Generated {len(completion_data)} completion records\")\n",
    "display(completion_df.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46f31d8b-7a89-4d47-9096-d85b8a5ae24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write completion data to Delta table\n",
    "completion_table = f\"{training_catalog}.{your_schema}.completion_data\"\n",
    "\n",
    "completion_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(completion_table)\n",
    "\n",
    "print(f\"✓ Created table: {completion_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcddb30-1fb9-4a44-a049-802e52c7427c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify all tables were created\n",
    "SHOW TABLES;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c983117a-2cc9-4fbf-802b-af0eb5d638c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 4: Data Cleansing and Quality (15 minutes)\n",
    "\n",
    "Real-world data often has quality issues. Let's explore common data cleansing operations including de-duplication and standardization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5b16ac8-ce97-4235-87fd-6fe036770ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Introduce Data Quality Issues\n",
    "\n",
    "Let's intentionally create some data quality problems to practice fixing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "155a8b28-aa93-42b5-8cf3-ca4518f2ed72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create duplicate records in production data\n",
    "production_with_issues = spark.table(production_table)\n",
    "\n",
    "# Get a sample of records to duplicate\n",
    "sample_records = production_with_issues.sample(0.05, seed=42)  # 5% duplicates\n",
    "\n",
    "# Add duplicates\n",
    "production_with_duplicates = production_with_issues.union(sample_records)\n",
    "\n",
    "# Introduce some data inconsistencies\n",
    "from pyspark.sql.functions import when, rand, col\n",
    "\n",
    "production_with_issues = production_with_duplicates.withColumn(\n",
    "    \"api_number\",\n",
    "    # Randomly add leading/trailing spaces to 10% of records\n",
    "    when(rand() < 0.1, concat(lit(\" \"), col(\"api_number\"), lit(\" \")))\n",
    "    .otherwise(col(\"api_number\"))\n",
    ").withColumn(\n",
    "    \"oil_volume_bbl\",\n",
    "    # Introduce some negative values (data errors)\n",
    "    when(rand() < 0.02, lit(-999))\n",
    "    .otherwise(col(\"oil_volume_bbl\"))\n",
    ")\n",
    "\n",
    "# Write to a new table\n",
    "dirty_table = f\"{training_catalog}.{your_schema}.production_data_dirty\"\n",
    "production_with_issues.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(dirty_table)\n",
    "\n",
    "print(f\"Created table with data quality issues: {dirty_table}\")\n",
    "print(f\"Original records: {production_with_issues.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c570410-99df-41e2-84e0-f61fd5daf536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Identify Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ee9055d-b969-49a6-b470-737bc5eff023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Count duplicates\n",
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "dirty_df = spark.table(dirty_table)\n",
    "\n",
    "# Check for exact duplicates\n",
    "duplicate_count = dirty_df.count() - dirty_df.distinct().count()\n",
    "print(f\"Exact duplicate records: {duplicate_count:,}\")\n",
    "\n",
    "# Check for duplicates based on key columns\n",
    "key_cols = [\"api_number\", \"production_date\"]\n",
    "duplicate_keys = dirty_df.groupBy(key_cols).count().filter(col(\"count\") > 1)\n",
    "print(f\"Duplicate keys (api_number + production_date): {duplicate_keys.count():,}\")\n",
    "\n",
    "display(duplicate_keys.orderBy(col(\"count\").desc()).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64137e5a-7419-417e-9e64-381b4dc20f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identify data standardization issues\n",
    "api_with_spaces = dirty_df.filter(\n",
    "    col(\"api_number\").startswith(\" \") | col(\"api_number\").endswith(\" \")\n",
    ").count()\n",
    "\n",
    "print(f\"API numbers with leading/trailing spaces: {api_with_spaces:,}\")\n",
    "\n",
    "# Check for negative values (invalid data)\n",
    "negative_oil = dirty_df.filter(col(\"oil_volume_bbl\") < 0).count()\n",
    "print(f\"Records with negative oil volume: {negative_oil:,}\")\n",
    "\n",
    "# Show examples\n",
    "display(dirty_df.filter(col(\"oil_volume_bbl\") < 0).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3489a2-fd1b-4574-9581-66361a82b00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise: De-duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f22a91a2-e193-4f1f-918e-ad18fe21275f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #TODO: Remove duplicate records keeping only the first occurrence\n",
    "# HINT: Use databricks assistant if you aren't sure what function to run\n",
    "\n",
    "# Your code here:\n",
    "cleaned_df = dirty_df  # Replace this line with your de-duplication logic\n",
    "\n",
    "print(f\"Original records: {dirty_df.count():,}\")\n",
    "print(f\"After de-duplication: {cleaned_df.count():,}\")\n",
    "print(f\"Removed: {dirty_df.count() - cleaned_df.count():,} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ed2b60-63dc-4323-b535-cf959fc0816f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SOLUTION De-Duplication"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution to de-duplication exercise\n",
    "cleaned_df = dirty_df.dropDuplicates([\"api_number\", \"production_date\"])\n",
    "\n",
    "print(f\"✓ De-duplication complete\")\n",
    "print(f\"Records after de-duplication: {cleaned_df.count():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72de63d9-e9e9-4f03-b609-89ec56a3f1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Exercise: Data Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c3fc311-68c1-4244-b428-43f14d640be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #TODO: Standardize API numbers by trimming whitespace and converting to uppercase\n",
    "# #TODO: Replace negative oil volumes with NULL values\n",
    "# HINT: Use databricks assistant if you aren't sure what function to run\n",
    "\n",
    "from pyspark.sql.functions import trim, when, col, upper\n",
    "\n",
    "# Your code here:\n",
    "standardized_df = cleaned_df  # Replace with your standardization logic\n",
    "\n",
    "\n",
    "# Verify the fixes\n",
    "api_with_spaces = standardized_df.filter(\n",
    "    col(\"api_number\").startswith(\" \") | col(\"api_number\").endswith(\" \")\n",
    ").count()\n",
    "\n",
    "negative_oil = standardized_df.filter(col(\"oil_volume_bbl\") < 0).count()\n",
    "\n",
    "print(f\"API numbers with spaces after cleaning: {api_with_spaces}\")\n",
    "print(f\"Negative oil volumes after cleaning: {negative_oil}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6548ff35-9512-4621-a16e-68e6e067aab6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SOLUTION Data Standardization"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution (uncomment to see):\n",
    "standardized_df = cleaned_df \\\n",
    "    .withColumn(\"api_number\", trim(col(\"api_number\"))) \\\n",
    "    .withColumn(\"oil_volume_bbl\", \n",
    "                when(col(\"oil_volume_bbl\") < 0, lit(None))\n",
    "                .otherwise(col(\"oil_volume_bbl\")))\n",
    "    \n",
    "display(standardized_df.limit(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c02d940-67d2-485c-b281-8bc670909241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Write Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f42ec5e-039c-4d02-8693-4747d7445524",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write cleaned data to new table\n",
    "clean_table = f\"{training_catalog}.{your_schema}.production_data_clean\"\n",
    "\n",
    "standardized_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(clean_table)\n",
    "\n",
    "print(f\"✓ Created clean table: {clean_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41e07a14-6a2e-489a-8791-30842849a04c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 5: Build a Data Product (15 minutes)\n",
    "\n",
    "Let's create a useful analytics table that combines data from multiple sources and performs aggregations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2b937f5-68c3-443e-b8fe-a8514e1b8b26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Create Well Production Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3a5f9c-30b0-4c4a-9877-2154c64a2475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Goal: Create a summary table showing total production by well with well details\n",
    "\n",
    "# Read the tables\n",
    "wells = spark.table(f\"{training_catalog}.{your_schema}.well_headers\")\n",
    "production = spark.table(clean_table)\n",
    "completions = spark.table(completion_table)\n",
    "\n",
    "# Aggregate production by well\n",
    "production_summary = production.groupBy(\"api_number\").agg(\n",
    "    sum(\"oil_volume_bbl\").alias(\"total_oil_bbl\"),\n",
    "    sum(\"gas_volume_mcf\").alias(\"total_gas_mcf\"),\n",
    "    sum(\"water_volume_bbl\").alias(\"total_water_bbl\"),\n",
    "    avg(\"gas_oil_ratio\").alias(\"avg_gor\"),\n",
    "    avg(\"water_cut_pct\").alias(\"avg_water_cut\"),\n",
    "    count(\"*\").alias(\"months_produced\"),\n",
    "    min(\"production_date\").alias(\"first_production\"),\n",
    "    max(\"production_date\").alias(\"last_production\")\n",
    ")\n",
    "\n",
    "display(production_summary.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5ae620-0762-491c-92ee-c3324d1e3531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Alias tables after join to avoid ambiguous columns\n",
    "well_production_summary = production_summary \\\n",
    "    .join(wells.alias(\"w\"), \"api_number\", \"inner\") \\\n",
    "    .join(completions.alias(\"c\"), \"api_number\", \"inner\") \\\n",
    "    .select(\n",
    "        col(\"api_number\"),\n",
    "        col(\"w.well_name\"),\n",
    "        col(\"w.operator\"),\n",
    "        col(\"w.well_type\"),\n",
    "        col(\"w.status\"),\n",
    "        col(\"w.target_formation\"),\n",
    "        col(\"c.completion_method\"),\n",
    "        col(\"c.num_stages\"),\n",
    "        col(\"w.total_depth_ft\"),\n",
    "        col(\"c.lateral_length_ft\"),\n",
    "        col(\"total_oil_bbl\"),\n",
    "        col(\"total_gas_mcf\"),\n",
    "        col(\"total_water_bbl\"),\n",
    "        col(\"avg_gor\"),\n",
    "        col(\"avg_water_cut\"),\n",
    "        col(\"months_produced\"),\n",
    "        col(\"first_production\"),\n",
    "        col(\"last_production\")\n",
    "    )\n",
    "\n",
    "well_production_summary = well_production_summary \\\n",
    "    .withColumn(\"total_boe\", \n",
    "        round(col(\"total_oil_bbl\") + col(\"total_gas_mcf\") / 6, 0)) \\\n",
    "    .withColumn(\"avg_monthly_oil\", \n",
    "        round(col(\"total_oil_bbl\") / col(\"months_produced\"), 1)) \\\n",
    "    .withColumn(\"avg_monthly_gas\", \n",
    "        round(col(\"total_gas_mcf\") / col(\"months_produced\"), 1))\n",
    "\n",
    "display(\n",
    "    well_production_summary\n",
    "    .orderBy(col(\"total_boe\").desc())\n",
    "    .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5325241-58f0-49ef-b0e1-13c36ac365a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #TODO: Calculate cumulative production per operator and avg oil/gas per well\n",
    "# HINT: Group by operator and sum/avg the production volumes\n",
    "\n",
    "# Your code here:\n",
    "operator_summary = None  # Replace with your aggregation logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61b87da-b417-40b0-b5f2-5f57bd9c42c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SOLUTION Production Summary"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Solution to operator summary exercise\n",
    "operator_summary = well_production_summary.groupBy(\"operator\").agg(\n",
    "    count(\"api_number\").alias(\"well_count\"),\n",
    "    sum(\"total_oil_bbl\").alias(\"operator_total_oil\"),\n",
    "    sum(\"total_gas_mcf\").alias(\"operator_total_gas\"),\n",
    "    round(avg(\"total_oil_bbl\"), 0).alias(\"avg_oil_per_well\"),\n",
    "    round(avg(\"total_gas_mcf\"), 0).alias(\"avg_gas_per_well\"),\n",
    "    round(sum(\"total_boe\"), 0).alias(\"operator_total_boe\")\n",
    ").orderBy(col(\"operator_total_boe\").desc())\n",
    "\n",
    "print(\"✓ Operator summary created\")\n",
    "display(operator_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c8d104-7de7-427a-a4f0-5d215e4c561f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save the data product as a managed table\n",
    "summary_table = f\"{training_catalog}.{your_schema}.well_production_summary\"\n",
    "\n",
    "well_production_summary.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(summary_table)\n",
    "\n",
    "print(f\"✓ Created data product: {summary_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d75941e-88d8-464e-94be-513d1f244cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 6: Visualizations and Charts (10 minutes)\n",
    "\n",
    "Databricks provides built-in visualization capabilities using the `display()` function. Let's create several charts to analyze our well data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec4cb371-7eed-4c50-a77e-b792db72acbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### Production by Well Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb6f8f6f-6f29-489a-9f32-a50489ca7191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Production comparison by well type\n",
    "SELECT \n",
    "    well_type,\n",
    "    COUNT(*) as well_count,\n",
    "    ROUND(SUM(total_oil_bbl), 0) as total_oil,\n",
    "    ROUND(SUM(total_gas_mcf), 0) as total_gas,\n",
    "    ROUND(AVG(total_oil_bbl), 0) as avg_oil_per_well,\n",
    "    ROUND(AVG(total_gas_mcf), 0) as avg_gas_per_well\n",
    "FROM well_production_summary\n",
    "GROUP BY well_type\n",
    "ORDER BY total_oil DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74f90f41-c898-4e8a-bc80-f6e571fe8b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Instructions to create visualization:**\n",
    "1. Run the cell above\n",
    "2. Click the **+** button above the results table\n",
    "3. Select **Visualization**\n",
    "4. Choose **Bar chart**\n",
    "5. Set X-axis: `well_type`\n",
    "6. Set Y-axis: `avg_oil_per_well` and `avg_gas_per_well`\n",
    "7. Click **Save**\n",
    "\n",
    "### Production Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5c3ba56-964c-4b58-b33e-adb22e15f703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Monthly production trends\n",
    "monthly_production = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('MONTH', production_date) as month,\n",
    "        SUM(oil_volume_bbl) as monthly_oil,\n",
    "        SUM(gas_volume_mcf) as monthly_gas,\n",
    "        SUM(water_volume_bbl) as monthly_water\n",
    "    FROM {clean_table}\n",
    "    GROUP BY DATE_TRUNC('MONTH', production_date)\n",
    "    ORDER BY month\n",
    "\"\"\")\n",
    "\n",
    "display(monthly_production)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97f57198-4be8-4d96-a53f-eabd3302960b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Instructions for line chart:**\n",
    "1. Click **+** above results → **Visualization**\n",
    "2. Choose **Line chart**\n",
    "3. X-axis: `month`\n",
    "4. Y-axis: `monthly_oil`, `monthly_gas` (you can select multiple)\n",
    "5. This shows production trends over time\n",
    "\n",
    "### Top Producing Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71e6800b-c6b6-48f7-a0f3-506db518f3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- #TODO: Write a query to find the top 10 wells by total BOE production\n",
    "-- HINT: Use ORDER BY and LIMIT\n",
    "\n",
    "SELECT \n",
    "    well_name,\n",
    "    operator,\n",
    "    target_formation,\n",
    "    total_boe,\n",
    "    total_oil_bbl,\n",
    "    total_gas_mcf\n",
    "FROM well_production_summary\n",
    "-- Add your ORDER BY and LIMIT here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dadc5936-a2a3-4fb8-80a2-84d03d4764a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SOLUTION Top Producing Wells"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Solution: Top 10 producing wells\n",
    "SELECT \n",
    "    well_name,\n",
    "    operator,\n",
    "    target_formation,\n",
    "    ROUND(total_boe, 0) as total_boe,\n",
    "    ROUND(total_oil_bbl, 0) as total_oil,\n",
    "    ROUND(total_gas_mcf, 0) as total_gas\n",
    "FROM well_production_summary\n",
    "ORDER BY total_boe DESC\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc460695-cb9c-487e-8e9e-8a8d0c6a04d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create a bar chart showing top producers:**\n",
    "1. Visualization type: **Bar chart**\n",
    "2. X-axis: `well_name`\n",
    "3. Y-axis: `total_boe`\n",
    "4. Color by: `operator`\n",
    "\n",
    "### Production by Formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8def2a5-ffc7-46bc-aad3-495af9c9e4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze production by target formation\n",
    "formation_analysis = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        target_formation,\n",
    "        COUNT(*) as well_count,\n",
    "        ROUND(SUM(total_boe), 0) as total_boe,\n",
    "        ROUND(AVG(total_boe), 0) as avg_boe_per_well,\n",
    "        ROUND(AVG(avg_gor), 1) as avg_gas_oil_ratio\n",
    "    FROM {summary_table}\n",
    "    GROUP BY target_formation\n",
    "    ORDER BY total_boe DESC\n",
    "\"\"\")\n",
    "\n",
    "display(formation_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1626ccac-8c06-4aca-9228-563d4d470e95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create a pie chart:**\n",
    "1. Visualization type: **Pie chart**\n",
    "2. Keys: `target_formation`\n",
    "3. Values: `total_boe`\n",
    "4. Shows percentage of total production by formation\n",
    "\n",
    "### Well Type Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33247180-fcfe-4a2d-bb4d-4d04764ebd7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Compare well performance metrics\n",
    "performance_by_type = well_production_summary.groupBy(\"well_type\", \"completion_method\").agg(\n",
    "    count(\"api_number\").alias(\"well_count\"),\n",
    "    round(avg(\"total_oil_bbl\"), 0).alias(\"avg_oil\"),\n",
    "    round(avg(\"total_gas_mcf\"), 0).alias(\"avg_gas\"),\n",
    "    round(avg(\"avg_water_cut\"), 1).alias(\"avg_water_cut_pct\")\n",
    ").orderBy(\"well_type\", col(\"well_count\").desc())\n",
    "\n",
    "display(performance_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c003e77b-f68d-49e4-a6c9-d203af396fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scatter Plot: Lateral Length vs Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ace0f9b-18b1-49f1-bce9-b10f1f651822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analyze relationship between lateral length and production for horizontal wells\n",
    "horizontal_wells = well_production_summary.filter(col(\"well_type\") == \"Horizontal\")\n",
    "\n",
    "scatter_data = horizontal_wells.select(\n",
    "    \"lateral_length_ft\",\n",
    "    \"total_oil_bbl\",\n",
    "    \"total_gas_mcf\",\n",
    "    \"num_stages\",\n",
    "    \"target_formation\"\n",
    ").filter(col(\"lateral_length_ft\") > 0)\n",
    "\n",
    "display(scatter_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51b687a2-e805-49be-9465-32b763ee8b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create a scatter plot:**\n",
    "1. Visualization type: **Scatter plot**\n",
    "2. X-axis: `lateral_length_ft`\n",
    "3. Y-axis: `total_oil_bbl`\n",
    "4. Color by: `target_formation`\n",
    "5. Shows correlation between lateral length and production\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a17e6d8d-ffc7-4796-a558-cb1e21afed1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Section 7: Best Practices and Key Takeaways (5 minutes)\n",
    "\n",
    "### Databricks Best Practices Summary\n",
    "\n",
    "**1. Data Organization:**\n",
    "- Always use Unity Catalog three-level namespace: `catalog.schema.table`\n",
    "- Use managed tables for new data (better performance and governance)\n",
    "- Create schemas for logical groupings (by team, project, or data domain)\n",
    "\n",
    "**2. Code Best Practices:**\n",
    "- Import PySpark functions explicitly: `from pyspark.sql.functions import col, sum, avg`\n",
    "- Use `display()` for interactive data exploration\n",
    "- Chain DataFrame transformations before calling actions\n",
    "- Avoid `collect()` on large datasets\n",
    "- Use Delta Lake format for ACID transactions and time travel\n",
    "\n",
    "**3. Performance Optimization:**\n",
    "- Run `OPTIMIZE` on tables regularly to compact small files\n",
    "- Use Z-ORDER for frequently queried columns: `OPTIMIZE table ZORDER BY (date, category)`\n",
    "- Use `cache()` for DataFrames accessed multiple times\n",
    "- Filter data early in your transformations\n",
    "\n",
    "**4. Data Quality:**\n",
    "- Always check for duplicates and handle them appropriately\n",
    "- Standardize data formats (trim whitespace, consistent casing)\n",
    "- Handle NULL values explicitly\n",
    "- Validate data ranges and constraints\n",
    "\n",
    "**5. Documentation:**\n",
    "- Use markdown cells to document your analysis\n",
    "- Add comments to complex code logic\n",
    "- Use meaningful variable and table names\n",
    "- Mark exercises and TODOs clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1065a62a-cd15-4f96-b431-7f7f8cc62e85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key Concepts Review\n",
    "\n",
    "**Magic Commands:**\n",
    "- `%sql` - Execute SQL queries\n",
    "- `%md` - Render markdown\n",
    "- `%fs` - Filesystem commands\n",
    "- `%sh` - Shell commands\n",
    "\n",
    "**DataFrame Operations:**\n",
    "- **Transformations** (lazy): `select()`, `filter()`, `groupBy()`, `join()`, `withColumn()`\n",
    "- **Actions** (trigger execution): `display()`, `count()`, `collect()`, `write()`\n",
    "\n",
    "**Unity Catalog:**\n",
    "- Three levels: Catalog → Schema → Table\n",
    "- Use fully qualified names: `catalog.schema.table`\n",
    "- Provides centralized governance and access control\n",
    "\n",
    "**Delta Lake:**\n",
    "- ACID transactions on data lakes\n",
    "- Time travel and versioning\n",
    "- Automatic schema evolution\n",
    "- Performance optimizations (OPTIMIZE, Z-ORDER, VACUUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc13465d-d67c-40be-97c6-874046c74e28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Additional Exercises to Try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91974d12-4a1c-49f0-8592-bc3899261dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# #TODO: Calculate the total production for each completion method\n",
    "# Compare multi-stage hydraulic fracturing vs other methods\n",
    "\n",
    "# #TODO: Identify wells with declining production (compare first 6 months vs last 6 months)\n",
    "\n",
    "# #TODO: Calculate water cut trends over time - does it increase as expected?\n",
    "\n",
    "# #TODO: Find the most productive formation by well type (horizontal vs vertical)\n",
    "\n",
    "# #TODO: Create a pivot table showing production by operator and formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "002266a7-60c7-408b-81a0-256e5851b54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Next Steps\n",
    "\n",
    "**Continue Learning:**\n",
    "1. Explore Delta Live Tables (DLT) for production pipelines\n",
    "2. Learn about Databricks SQL for ad-hoc analysis\n",
    "3. Study performance tuning and optimization\n",
    "4. Practice with real datasets from your organization\n",
    "5. Learn about ML integration with MLflow\n",
    "\n",
    "**Documentation Resources:**\n",
    "- Databricks Documentation: [docs.databricks.com](https://docs.databricks.com)\n",
    "- PySpark API Reference: [spark.apache.org/docs/latest/api/python](https://spark.apache.org/docs/latest/api/python)\n",
    "- Delta Lake Documentation: [docs.delta.io](https://docs.delta.io)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37d03310-3881-455e-a627-8b075f0ad836",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "When you're finished with this training, you can optionally clean up your schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "959a491e-0306-461e-944d-1553ee50be4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: This will delete all tables in your schema\n",
    "# Uncomment only if you want to clean up\n",
    "\n",
    "spark.sql(f\"DROP SCHEMA IF EXISTS training.{your_schema} CASCADE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40054829-c853-480d-a945-144ddd455f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Congratulations!** You've completed the Databricks beginner training. You now know how to:\n",
    "\n",
    "✓ Use Databricks notebooks with magic commands and markdown\n",
    "✓ Work with Unity Catalog's three-level namespace\n",
    "✓ Create schemas and Delta tables\n",
    "✓ Generate and manipulate data with PySpark\n",
    "✓ Perform data cleansing (de-duplication and standardization)\n",
    "✓ Build data products with joins and aggregations\n",
    "✓ Create visualizations and charts using `display()`\n",
    "✓ Follow Databricks best practices\n",
    "\n",
    "Keep practicing with your own datasets and explore more advanced features as you grow your skills!\n",
    "\n",
    "---\n",
    "\n",
    "**Training Complete** | Version 1.0 | October 2025"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Training Notebook",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
